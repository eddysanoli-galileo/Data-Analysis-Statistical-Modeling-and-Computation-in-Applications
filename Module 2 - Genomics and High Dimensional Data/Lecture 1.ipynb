{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1 - Visualization of High Dimensional Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nptyping import NDArray, Int, Shape\n",
    "from sympy import symbols, init_printing, solve, Array, Symbol, linsolve, Sum\n",
    "from sympy import tensorproduct as tp\n",
    "import sympy as sp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Expectation and Covariance of a Random Vector\n",
    "\n",
    "#### Covariance Matrix of a Random Vector\n",
    "\n",
    "![Alt text](Images/covariance_random_vector.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Review: Empirical Mean and Covariance Matrix of a Vector Data Set I\n",
    "\n",
    "#### Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix\n",
    "X: NDArray[Shape[\"4, 3\"], Int] = np.array([\n",
    "    [8, 4, 7],\n",
    "    [2, 8, 1],\n",
    "    [3, 1, 1],\n",
    "    [9, 7, 4]\n",
    "])\n",
    "\n",
    "# Number of samples\n",
    "n: int = X.shape[0]\n",
    "\n",
    "# Number of features\n",
    "d: int = X.shape[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Formula for the Vector Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually Calculated Sample Mean:  [5.5  5.   3.25]\n",
      "Sample Mean with Matrix Multiplications:  [[5.5 ]\n",
      " [5.  ]\n",
      " [3.25]]\n",
      "The value of A in the problem is X.T / n\n"
     ]
    }
   ],
   "source": [
    "# Get the mean of each row\n",
    "print(\"Manually Calculated Sample Mean: \", X.mean(axis=0))\n",
    "\n",
    "# Column vector of ones\n",
    "ones = np.ones((n, 1))\n",
    "\n",
    "# Try to get the same result but using matrix multiplication. All elements\n",
    "# next to the \"ones\" array will consist of the A matrix\n",
    "print(\"Sample Mean with Matrix Multiplications: \", (X.T @ ones) / n)\n",
    "print(\"The value of A in the problem is X.T / n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Sample Covariance for a Data Set of Vector\n",
    "\n",
    "![Alt text](Images/sample_covariance.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum: \n",
      "[[158 114  97]\n",
      " [114 130  65]\n",
      " [ 97  65  67]]\n",
      "Empirical Covariance: \n",
      "[[9.25   1.     6.375 ]\n",
      " [1.     7.5    0.    ]\n",
      " [6.375  0.     6.1875]]\n"
     ]
    }
   ],
   "source": [
    "# Re-calculate the sample mean\n",
    "sample_mean = X.T @ ones / n\n",
    "\n",
    "# Calculate the sum\n",
    "sum = 0\n",
    "for i in range(n):\n",
    "\n",
    "    # Turn each Xi into a column vector\n",
    "    # (At the beginning, it is said that each Xi is a column vector)\n",
    "    Xi: NDArray[Shape[\"3, 1\"], Int] = X[i, :].reshape(d, 1)\n",
    "\n",
    "    # Process the sum\n",
    "    sum += Xi @ Xi.T\n",
    "\n",
    "print(\"Sum: \")\n",
    "print(sum)\n",
    "\n",
    "# Calculate the empirical covariance\n",
    "empirical_covariance = ((1/n) * sum) - (sample_mean @ sample_mean.T)\n",
    "print(\"Empirical Covariance: \")\n",
    "print(empirical_covariance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formula for the Empirical Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1:  False\n",
      "Option 2:  True\n",
      "Option 3:  False\n",
      "Option 4:  False\n"
     ]
    }
   ],
   "source": [
    "# Identity matrix of n x n and d x d\n",
    "In = np.identity(n)\n",
    "Id = np.identity(d)\n",
    "\n",
    "# Ones column vector\n",
    "ones = np.ones((n, 1))\n",
    "\n",
    "# ================== OPTION 1 ================== #\n",
    "\n",
    "option_1 = (1/n) * X.T @ (In - ones.T @ ones) @ X\n",
    "print(\"Option 1: \", np.array_equal(option_1, empirical_covariance))\n",
    "\n",
    "# ================== OPTION 2 ================== #\n",
    "\n",
    "option_2 = (1/n) * X.T @ (In - (1/n) * ones @ ones.T) @ X\n",
    "print(\"Option 2: \", np.array_equal(option_2, empirical_covariance))\n",
    "\n",
    "# ================== OPTION 3 ================== #\n",
    "\n",
    "# Fails due to dimensional reasons. \n",
    "# The matrix multiplication is not possible\n",
    "try:\n",
    "    option_3 = (1/n) * X @ (Id - (1/d) * ones @ ones.T) @ X.T\n",
    "except:\n",
    "    print(\"Option 3: \", False)\n",
    "\n",
    "# ================== OPTION 4 ================== #\n",
    "\n",
    "# Fails due to dimensional reasons.\n",
    "# The matrix multiplication is not possible\n",
    "try:\n",
    "    option_4 = (1/n) * X @ (In - (1/n) * ones @ ones.T) @ X.T\n",
    "except:\n",
    "    print(\"Option 4: \", False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Empirical Mean and Covariance Matrix of a Vector Data Set II\n",
    "\n",
    "#### Matrix Products Involving Outer Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "# Vector where the sum of the elements is 0\n",
    "x: NDArray[Shape[\"3,1\"], Int] = np.array([\n",
    "    [1],\n",
    "    [2],\n",
    "    [-3],\n",
    "])\n",
    "\n",
    "# Column unit vector\n",
    "ones = np.ones((3, 1))\n",
    "\n",
    "# What is this expression equal to?\n",
    "print((ones @ ones.T) @ x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An Orthogonal Projection Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hx:  [ 2.33333333 -0.66666667 -1.66666667]\n",
      "H^2x:  [ 2.33333333 -0.66666667 -1.66666667]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([2, -1, -2]).T\n",
    "\n",
    "# Identity matrix and unit column vector\n",
    "In = np.identity(3)\n",
    "ones = np.ones((3, 1))\n",
    "\n",
    "# Calculate H\n",
    "H = In - (1/3) * ones @ ones.T\n",
    "\n",
    "# Get Hx\n",
    "print(\"Hx: \", H @ x)\n",
    "\n",
    "# Get H^2x\n",
    "print(\"H^2x: \", H @ H @ x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Measuring the Spread of a Point Cloud\n",
    "\n",
    "#### Projection onto a Line\n",
    "\n",
    "![Alt text](Images/vector_projection.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes\n",
      "x1:  (2, 1)\n",
      "u:  (2, 1)\n",
      "\n",
      "Projection of x1 onto u:  [[1.]\n",
      " [2.]]\n",
      "Projection of x2 onto u:  [[2.2]\n",
      " [4.4]]\n",
      "Projection of x3 onto u:  [[-0.2]\n",
      " [-0.4]]\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "d = 2\n",
    "\n",
    "# The vector onto which we project\n",
    "u = np.array([[1, 2]]).T * (1/np.sqrt(5))\n",
    "\n",
    "# Dataset\n",
    "x1 = np.array([[1, 2]]).T\n",
    "x2 = np.array([[3, 4]]).T\n",
    "x3 = np.array([[-1, 0]]).T\n",
    "\n",
    "# See shapes\n",
    "print(\"Shapes\")\n",
    "print(\"x1: \", x1.shape)\n",
    "print(\"u: \", u.shape)\n",
    "print()\n",
    "\n",
    "# Find the projection of each sample onto u\n",
    "print(\"Projection of x1 onto u: \", (u @ x1.T) @ u)\n",
    "print(\"Projection of x2 onto u: \", (u @ x2.T) @ u)\n",
    "print(\"Projection of x3 onto u: \", (u @ x3.T) @ u)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Empirical Variance of a Data Set in a Given Direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Signed distances:  [ 2.23606798  4.91934955 -0.4472136 ]\n",
      "Empirical Variance:  4.8\n",
      "u^T S u:  [[4.8]]\n"
     ]
    }
   ],
   "source": [
    "# Empirical variance: The signed distance from the origin to the \n",
    "# endpoint of the projection\n",
    "\n",
    "# Get the norm of all the u * Xi \n",
    "signed_distances = np.array([\n",
    "    (u.T @ x1).item(),\n",
    "    (u.T @ x2).item(),\n",
    "    (u.T @ x3).item()\n",
    "])\n",
    "print(\"Signed distances: \", signed_distances)\n",
    "\n",
    "# Get the empirical variance\n",
    "print(\"Empirical Variance: \", np.var(signed_distances))\n",
    "\n",
    "# Build a matrix with all the samples\n",
    "X = np.vstack((x1.T, x2.T, x3.T))\n",
    "\n",
    "# Re-calculate n and d\n",
    "n = X.shape[0]\n",
    "d = X.shape[1]\n",
    "\n",
    "# Identity matrix and column vector of ones\n",
    "In = np.identity(n)\n",
    "ones = np.ones((n, 1))\n",
    "\n",
    "# Calculate the empirical covariance\n",
    "S = (1/n) * X.T @ (In - (1/n) * ones @ ones.T) @ X\n",
    "\n",
    "# Get u^T S u\n",
    "print(\"u^T S u: \", (u.T @ S @ u))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. The Decomposition Theorem for Symmetric Matrices\n",
    "\n",
    "#### Concept Check: Othogonal Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P P^T = Id:  True\n",
      "P^T P = Id:  True\n",
      "P is orthogonal\n",
      "v1 * v2:  0.0\n"
     ]
    }
   ],
   "source": [
    "# Identity matrix of d x d\n",
    "d = 3\n",
    "Id = np.identity(d)\n",
    "\n",
    "# Check that the matrix is orthogonal \n",
    "condition_1 = np.array_equal(Id @ Id.T, Id)\n",
    "condition_2 = np.array_equal(Id.T @ Id, Id)\n",
    "print(\"P P^T = Id: \", condition_1)\n",
    "print(\"P^T P = Id: \", condition_2)\n",
    "\n",
    "# Is P orthogonal?\n",
    "if condition_1 and condition_2:\n",
    "    print(\"P is orthogonal\")\n",
    "else:\n",
    "    print(\"P is not orthogonal\")\n",
    "\n",
    "# What is v1 * v2?\n",
    "v1 = Id[:, 0]\n",
    "v2 = Id[:, 1]\n",
    "print(\"v1 * v2: \", v1 @ v2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eigenvectors and Eigenvalues of a Decomposition of a Symmetric Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix([[1.0*lambda*v1_1], [1.0*lambda*v1_2]])\n",
      "This is equal to A * v1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sympy import Matrix\n",
    "\n",
    "Id = np.identity(2)\n",
    "P = Id.copy()\n",
    "\n",
    "# Declare the positive value for lambda\n",
    "lamb = Symbol(\"lambda\", positive=True)\n",
    "\n",
    "# Get the diagonal matrix of positive lambdas\n",
    "D = Matrix([\n",
    "    [lamb,    0],\n",
    "    [   0, lamb]\n",
    "])\n",
    "\n",
    "# Get the value of PDP^T\n",
    "A = tp(tp(P, D), P.T)\n",
    "\n",
    "# Declare the elements of the v1 vector\n",
    "v1_1, v1_2 = symbols(\"v1_1, v1_2\")\n",
    "\n",
    "# Vi is the ith column of P. V1 is the first column\n",
    "v1 = Matrix([\n",
    "    [v1_1],\n",
    "    [v1_2]\n",
    "])\n",
    "\n",
    "# Get the value of A * v1\n",
    "print(A * v1)\n",
    "print(\"This is equal to A * v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept Check: The Decomposition Theorem for Symmetric Matrices\n",
    "\n",
    "![Alt text](Images/decomposition_theorem.PNG)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Principal Component Analysis (PCA)\n",
    "\n",
    "#### Centering Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H:  [[ 0.5 -0.5]\n",
      " [-0.5  0.5]]\n",
      "H^2:  [[ 0.5 -0.5]\n",
      " [-0.5  0.5]]\n",
      "H^2 = H\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Dimensions\n",
    "n = 2\n",
    "\n",
    "# Identity matrix of n x n and column vector of ones\n",
    "In = np.identity(n)\n",
    "ones = np.ones((n, 1))\n",
    "\n",
    "# Calculate the matrix H\n",
    "H = In - (1/n) * ones @ ones.T\n",
    "print(\"H: \", H)\n",
    "\n",
    "# Calculate its square\n",
    "H2 = H @ H\n",
    "print(\"H^2: \", H2)\n",
    "\n",
    "# Is H^2 = H?\n",
    "if np.array_equal(H2, H):\n",
    "    print(\"H^2 = H\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"H^2 != H\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:  [[0. 0.]\n",
      " [0. 2.]]\n",
      "\n",
      "Eigenvalues:  [0. 2.]\n",
      "Eigenvectors:  [[1. 0.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# Sample data (centered around 0 with a sample mean of 0 in both dimensions)\n",
    "# X = [\n",
    "#   [x1, y1],\n",
    "#   [x2, y2]\n",
    "# ]\n",
    "X = np.array([\n",
    "    [0, 1],\n",
    "    [0, -1]\n",
    "])\n",
    "\n",
    "# We calculate H (the projection matrix)\n",
    "n = X.shape[0]\n",
    "ones = np.ones((n, 1))\n",
    "In = np.identity(n)\n",
    "H = In - (1/n) * ones @ ones.T\n",
    "\n",
    "# We calculate the empirical covariance matrix\n",
    "# Note: Here we can use either \"n-1\" or \"n\" as the denominator for the first element.\n",
    "# By using a different denominator you are simply scaling the result by a constant, so\n",
    "# the result will be the same ultimately the same.\n",
    "S = (1/(n-1)) * X.T @ H @ X\n",
    "print(\"S: \", S)\n",
    "print()\n",
    "\n",
    "# Find the eigenvalues and eigenvectors of S\n",
    "# (In other words, find pairs of scalar lambda and vector v such that Sv = lambda v)\n",
    "eigenvalues, eigenvectors = np.linalg.eig(S)\n",
    "print(\"Eigenvalues: \", eigenvalues)\n",
    "print(\"Eigenvectors: \", eigenvectors)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Conceptual Examples in 2 Dimensions\n",
    "\n",
    "#### Conceptual Example II: 2 Data points in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:  [[2.  1. ]\n",
      " [1.  0.5]]\n",
      "Eigenvalues:  [2.5 0. ]\n",
      "Solutions for v1 {(2.0*v1_2, v1_2)}\n",
      "Solutions for v2 {(-0.5*v2_2, v2_2)}\n"
     ]
    }
   ],
   "source": [
    "# Data set (centered around 0 with a sample mean of 0 in both dimensions)\n",
    "X = np.array([\n",
    "    [ 1,  1/2],\n",
    "    [-1, -1/2]\n",
    "])\n",
    "\n",
    "# Find the projection matrix (H)\n",
    "n = X.shape[0]\n",
    "ones = np.ones((n, 1))\n",
    "In = np.identity(n)\n",
    "H = In - (1/n) * ones @ ones.T\n",
    "\n",
    "# Calculate the empirical covariance matrix (S)\n",
    "S = (1/(n-1)) * X.T @ H @ X\n",
    "print(\"S: \", S)\n",
    "\n",
    "# Find the eigenvalues of S\n",
    "eigenvalues, _ = np.linalg.eig(S)\n",
    "print(\"Eigenvalues: \", eigenvalues)\n",
    "\n",
    "# Symbols\n",
    "v1_1, v1_2 = symbols(\"v1_1, v1_2\")\n",
    "v2_1, v2_2 = symbols(\"v2_1, v2_2\")\n",
    "\n",
    "# Symbolic eigenvectors\n",
    "v1 = Matrix([\n",
    "    [v1_1],\n",
    "    [v1_2]\n",
    "])\n",
    "v2 = Matrix([\n",
    "    [v2_1],\n",
    "    [v2_2]\n",
    "])\n",
    "\n",
    "# Find the set of equations that define the first eigenvector\n",
    "# (S - lambda_1 * Id) * v_1 = 0\n",
    "solutions1 = linsolve((S - eigenvalues[0] * In) * v1, (v1_1, v1_2))\n",
    "print(\"Solutions for v1\", solutions1)\n",
    "\n",
    "# Find the second eigenvector by solving the following equation:\n",
    "# (S - lambda_2 * Id) * v_2 = 0\n",
    "solutions2 = linsolve((S - eigenvalues[1] * In) * v2, (v2_1, v2_2))\n",
    "print(\"Solutions for v2\", solutions2)\n",
    "\n",
    "# NOTE: The equations above dont have an exact solution, they have actually an infinite\n",
    "# set of solutions. This is because the eigenvectors are not unique. The thing that we\n",
    "# need to pay attention is the relationship between each of the components in a solution.\n",
    "# For example, above we have solutions for \n",
    "#   - v1: {2v1_2, v1_2} which means that the eigenvector is a scalar multiple of [2, 1].\n",
    "#   - v2: {-0,5v2_2, v2_2} which means that the eigenvector is a scalar multiple of [-0.5, 1]\n",
    "#     or [1, -2] if we divide by -0.5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Project onto PC1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection of the data points onto PC1:  [[ 1.11803399 -1.11803399]]\n",
      "First element is y1 (projection of x1 onto PC1) and second element is y2\n"
     ]
    }
   ],
   "source": [
    "# Get the first principal component defined above\n",
    "PC1 = np.array([[2, 1]]).T\n",
    "\n",
    "# Normalize the first principal component\n",
    "PC1 = PC1 / np.linalg.norm(PC1)\n",
    "\n",
    "# Get the projection of x1 and x2 onto the first principal \n",
    "# component (y1 and y2)\n",
    "y = PC1.T @ X.T\n",
    "print(\"Projection of the data points onto PC1: \", y)\n",
    "print(\"First element is y1 (projection of x1 onto PC1) and second element is y2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. (Optional) Conceptual Examples in 2 Dimensions\n",
    "\n",
    "#### Conceptual Example III: 3 Data points in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:  [[ 1.00000000e+00  0.00000000e+00]\n",
      " [-1.11022302e-16  3.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# The 3 2D data points\n",
    "X = np.array([\n",
    "    [ 0,  2],\n",
    "    [ 1, -1],\n",
    "    [-1, -1]\n",
    "])\n",
    "\n",
    "# Find the projection matrix (H)\n",
    "n = X.shape[0]\n",
    "ones = np.ones((n, 1))\n",
    "In = np.identity(n)\n",
    "H = In - (1/n) * ones @ ones.T\n",
    "\n",
    "# Calculate the empirical covariance matrix (S)\n",
    "S = (1/(n-1)) * X.T @ H @ X\n",
    "print(\"S: \", S)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Decomposition of the Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:  [3. 1.]\n",
      "Normalized eigenvectors:  [[0.00000000e+00 1.00000000e+00]\n",
      " [1.00000000e+00 5.55111512e-17]]\n",
      "\n",
      "Manual calculation\n",
      "==================\n",
      "Solutions for v1 {(0, v1_2)}\n",
      "Solutions for v2 {(1.8014398509482e+16*v2_2, v2_2)}\n",
      "Eigenvector 1 (based on solutions1): [0, 1]\n",
      "Eigenvector 2 (based on solutions2): [1, 0]\n"
     ]
    }
   ],
   "source": [
    "# Get the eigenvalues of S\n",
    "eigenvalues, eigenvectors = np.linalg.eig(S)\n",
    "print(\"Eigenvalues: \", eigenvalues)\n",
    "print(\"Normalized eigenvectors: \", eigenvectors)\n",
    "print()\n",
    "\n",
    "# Symbols\n",
    "v1_1, v1_2, v2_1, v2_2 = symbols(\"v1_1, v1_2, v2_1, v2_2\")\n",
    "v1 = Matrix([\n",
    "    [v1_1],\n",
    "    [v1_2]\n",
    "])\n",
    "v2 = Matrix([\n",
    "    [v2_1],\n",
    "    [v2_2]\n",
    "])\n",
    "\n",
    "print(\"Manual calculation\")\n",
    "print(\"==================\")\n",
    "\n",
    "# Identity matrix of d x d\n",
    "d = X.shape[1]\n",
    "Id = np.identity(d)\n",
    "\n",
    "# Find the set of equations that define the first eigenvector\n",
    "# (S - lambda_1 * Id) * v_1 = 0\n",
    "solutions1 = linsolve((S - eigenvalues[0] * Id) * v1, (v1_1, v1_2))\n",
    "print(\"Solutions for v1\", solutions1)\n",
    "\n",
    "# Find the second eigenvector by solving the following equation:\n",
    "# (S - lambda_2 * Id) * v_2 = 0\n",
    "solutions2 = linsolve((S - eigenvalues[1] * Id) * v2, (v2_1, v2_2))\n",
    "print(\"Solutions for v2\", solutions2)\n",
    "\n",
    "# Print the eigenvectors\n",
    "print(\"Eigenvector 1 (based on solutions1): [0, 1]\")\n",
    "print(\"Eigenvector 2 (based on solutions2): [1, 0]\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projection of the data points onto PC1:  [[ 2. -1. -1.]]\n",
      "Sample variance of y:  2.0\n"
     ]
    }
   ],
   "source": [
    "# First principal component (normalized)\n",
    "PC1 = np.array([[0, 1]]).T\n",
    "PC1 = PC1 / np.linalg.norm(PC1)\n",
    "\n",
    "# Calculate y1, y2, and y3, the signed length of the projections of the\n",
    "# three data points onto the first principal component. Remember that the\n",
    "# signed length of a projection is just \"u * x\" instead of \"(u * x) * u\" for\n",
    "# a projection\n",
    "y = PC1.T @ X.T\n",
    "print(\"Projection of the data points onto PC1: \", y)\n",
    "\n",
    "# Sample variance of y\n",
    "var_y = np.var(y)\n",
    "print(\"Sample variance of y: \", var_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following section is done to prove that the transition matrix from X to the space formed by PC1 and PC2 consists of each of the normalized eigenvectors of S as a row of the matrix. This is part of the proof in the \"output of PCA\" section of part 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transition matrix [[0. 1.]\n",
      " [1. 0.]]\n",
      "Projection onto the (PC1, PC2) space:  [[ 2. -1. -1.]\n",
      " [ 0.  1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "PC2 = np.array([[1, 0]]).T\n",
    "PC2 = PC2 / np.linalg.norm(PC2)\n",
    "\n",
    "# Transition matrix to have data points in PC1 and PC2\n",
    "T = np.concatenate((PC1, PC2), axis=1)\n",
    "print(\"Transition matrix\", T)\n",
    "\n",
    "# Project the data points onto PC1 and PC2\n",
    "Y = T.T @ X.T\n",
    "print(\"Projection onto the (PC1, PC2) space: \", Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Conceptual Examples Continued\n",
    "\n",
    "#### Conceptual Example IV: 4 Data points in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S:  [[0.5 0.5]\n",
      " [0.5 2.5]]\n"
     ]
    }
   ],
   "source": [
    "# 4 2D data points\n",
    "X = np.array([\n",
    "    [ 0,  2],\n",
    "    [ 0, -2],\n",
    "    [ 1,  1],\n",
    "    [-1, -1],\n",
    "])\n",
    "\n",
    "# Get the projection matrix\n",
    "n = X.shape[0]\n",
    "ones = np.ones((n, 1))\n",
    "In = np.identity(n)\n",
    "H = In - (1/n) * ones @ ones.T \n",
    "\n",
    "# Calculate the empirical covariance matrix\n",
    "# Note: you can also use (1/n) here and the results will be simpler and equally valid\n",
    "S = (1/(n)) * X.T @ H @ X\n",
    "print(\"S: \", S)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spectral Decomposition of the Covariance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues:  [0.38196601 2.61803399]\n",
      "Normalized eigenvectors:  [[-0.97324899 -0.22975292]\n",
      " [ 0.22975292 -0.97324899]]\n",
      "\n",
      "Manual calculation\n",
      "==================\n",
      "Solutions for v1: {(0, 0)}\n",
      "Solutions for v2 {(0, 0)}\n"
     ]
    }
   ],
   "source": [
    "# Get the eigenvalues of S\n",
    "eigenvalues, eigenvectors = np.linalg.eig(S)\n",
    "print(\"Eigenvalues: \", eigenvalues)\n",
    "print(\"Normalized eigenvectors: \", eigenvectors)\n",
    "print()\n",
    "\n",
    "# Symbols\n",
    "v1_1, v1_2, v2_1, v2_2 = symbols(\"v1_1, v1_2, v2_1, v2_2\")\n",
    "v1 = Matrix([\n",
    "    [v1_1],\n",
    "    [v1_2]\n",
    "])\n",
    "v2 = Matrix([\n",
    "    [v2_1],\n",
    "    [v2_2]\n",
    "])\n",
    "\n",
    "print(\"Manual calculation\")\n",
    "print(\"==================\")\n",
    "\n",
    "# Identity matrix of d x d\n",
    "d = X.shape[1]\n",
    "Id = np.identity(d)\n",
    "\n",
    "# Find the set of equations that define the first eigenvector\n",
    "# (S - lambda_1 * Id) * v_1 = 0\n",
    "solutions1 = linsolve((S - eigenvalues[0] * Id) * v1, (v1_1, v1_2))\n",
    "print(\"Solutions for v1:\", solutions1)\n",
    "\n",
    "# Find the second eigenvector by solving the following equation:\n",
    "# (S - lambda_2 * Id) * v_2 = 0\n",
    "solutions2 = linsolve((S - eigenvalues[1] * Id) * v2, (v2_1, v2_2))\n",
    "print(\"Solutions for v2\", solutions2)\n",
    "\n",
    "# Note: Sympy was unable to find the solutions for the eigenvectors,\n",
    "# so the solution was to use the eigenvectors from wolfram alpha:\n",
    "# v1 = [-2 + sqrt(5), 1] and v2 = [-2 - sqrt(5), 1]\n",
    "# \n",
    "# Link: https://www.wolframalpha.com/input?i2d=true&i=%7B%7BDivide%5B2%2C3%5D%2CDivide%5B2%2C3%5D%7D%2C%7BDivide%5B2%2C3%5D%2CDivide%5B10%2C3%5D%7D%7D"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Covariance versus Correlation\n",
    "\n",
    "![Alt text](Images/correlation_random_vars.PNG)\n",
    "\n",
    "![Alt text](Images/covariance-based-pca-vs-correlation-based-pca.PNG)\n",
    "\n",
    "### 14. Total Variance\n",
    "\n",
    "![Alt text](Images/total_variance.PNG)\n",
    "\n",
    "Response from an assistant professor:\n",
    "A correlation matrix has ones down the diagonal, so nothing interesting is happening there. Instead, PCA can be respond to off-diagonal elements, looking for some direction—not an axis—where strong correlations lead to higher variance. (Consider a two-dimensional case with perfectly correlated variables. PC1 would be the  diagonal.)\n",
    "\n",
    "But what if all the variables are independent? The correlation matrix is . All eigenvalues are one, every vector is an eigenvector. No direction is better than any other!\n",
    "\n",
    "In contrast, PCA can still do something useful with the covariance matrix in this case. PC1 will be along an axis, in the direction of whichever variable has the greatest variance. (I think this is a counterexample to a point from another thread, where if we normalize the data, we can't recover what PCA could have told us about the unnormalized data. It also illustrates a problem with generating all our test cases randomly: What are the chances we get a diagonal matrix? But that's an important corner case!)\n",
    "\n",
    "### 15. Multidimensional Scaling (MDS)\n",
    "\n",
    "![Alt text](Images/dimensionality_reduction_methods.PNG)\n",
    "\n",
    "- distance matrix: Distance measure between points. Euclidean distance, manhattan distance, etc.\n",
    "- dissimilarity matrix: Similarity measure between points. 1 - correlation, 1 - cosine similarity, etc.\n",
    "- In the end MDS is just like PCA, but instead of using the covariance matrix to find the eigenvectors, we use the matrix $X X^T$. Due to the way how classical PCA operates, it technically operates on $X^T X$ (used for the covariance matrix).\n",
    "\n",
    "![Alt text](Images/gram_matrix.PNG)\n",
    "\n",
    "### 16. Solving MDS\n",
    "\n",
    "#### Implementation of Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gram matrix:  [[ 2  0  0]\n",
      " [ 0  2 -2]\n",
      " [ 0 -2  2]]\n",
      "Eigenvalues:  [4.0000000e+00 4.4408921e-16 2.0000000e+00]\n",
      "Normalized eigenvectors:  [[ 0.          0.          1.        ]\n",
      " [-0.70710678  0.70710678  0.        ]\n",
      " [ 0.70710678  0.70710678  0.        ]]\n",
      "\n",
      "Sigma 1:  [[4.]]\n",
      "V 1:  [[ 0.        ]\n",
      " [-0.70710678]\n",
      " [ 0.70710678]]\n",
      "Solution to the MDS problem:  [[ 0.        ]\n",
      " [-1.41421356]\n",
      " [ 1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "# 3 2D data points\n",
    "X = np.array([\n",
    "    [1, 1],\n",
    "    [1, -1],\n",
    "    [-1, 1]\n",
    "])\n",
    "\n",
    "# Get the gram matrix\n",
    "B = X @ X.T\n",
    "print(\"Gram matrix: \", B)\n",
    "\n",
    "# Find the eigenvalues and eigenvectors of the gram matrix\n",
    "# Note: Each column in the eigenvectors matrix is an eigenvector\n",
    "eigenvalues, eigenvectors = np.linalg.eig(B)\n",
    "print(\"Eigenvalues: \", eigenvalues)\n",
    "print(\"Normalized eigenvectors: \", eigenvectors)\n",
    "print()\n",
    "\n",
    "# We construct the diagonal matrix \"sigma 1\" by choosing the\n",
    "# q largest (q = 1 in this case, since that is the target dimensionality)\n",
    "# eigenvalues and putting them in a diagonal matrix\n",
    "sigma_1 = np.diag(eigenvalues[:1])\n",
    "print(\"Sigma 1: \", sigma_1)\n",
    "\n",
    "# We construct the matrix V_1 by choosing the q largest eigenvectors\n",
    "# (V1 = [v1, v2, ..., vq], where v1 is a column vector)\n",
    "V_1 = np.reshape(eigenvectors[:, 0], (3, 1))\n",
    "print(\"V 1: \", V_1)\n",
    "\n",
    "# Get the solution to the MDS problem (Y = V_1 * (sigma_1)^1/2)\n",
    "Y = V_1 @ np.sqrt(sigma_1)\n",
    "print(\"Solution to the MDS problem: \", Y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Stochastic Neighbor Embedding (SNE) and t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "![Alt text](Images/sne_p_ij.PNG)\n",
    "![Alt text](Images/sne_q_ij.PNG)\n",
    "\n",
    "#### First Example: Distribution in the High Dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of all p_ij:  3*exp(-A)\n",
      "p_12 = p_13 = p_23:  1/3\n"
     ]
    }
   ],
   "source": [
    "# All points are at a distance \"A\" of each other\n",
    "A = Symbol(\"A\", positive=True)\n",
    "\n",
    "# Create the distance matrix\n",
    "D = Matrix([\n",
    "    [0, A, A],\n",
    "    [A, 0, A],\n",
    "    [A, A, 0]\n",
    "])\n",
    "\n",
    "# We need to calculate all p_ij: p_12, p_13, p_23\n",
    "# All other p_ij are just the transpose of the other p_ij, so we just use the lower half\n",
    "D_lower = D.lower_triangular()\n",
    "\n",
    "from sympy import exp\n",
    "\n",
    "# We get the sum of all non-zero elements in the lower triangular matrix\n",
    "# (i.e. the sum of all p_ij)\n",
    "total_sum = 0\n",
    "for i in range(D_lower.shape[0]):\n",
    "    for j in range(D_lower.shape[1]):\n",
    "        if i > j:\n",
    "            total_sum += exp(-D_lower[i, j])\n",
    "\n",
    "# Print the sum of all p_ij\n",
    "print(\"Sum of all p_ij: \", total_sum)\n",
    "\n",
    "# Calculate p_12, p_13, p_23 (all are the same)\n",
    "p_12 = exp(-A) / total_sum\n",
    "print(\"p_12 = p_13 = p_23: \", p_12)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Embedding three points into 1 dimension\n",
    "\n",
    "#### Minimize KL\n",
    "\n",
    "![Alt text](Images/minimize_kl.PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_12:  0.42231879825151825\n",
      "p_13:  0.42231879825151825\n",
      "p_23:  0.15536240349696354\n",
      "q_12:  exp(-a**2)/(2*exp(-a**2) + exp(-4*a**2))\n",
      "q_13:  exp(-a**2)/(2*exp(-a**2) + exp(-4*a**2))\n",
      "q_23:  exp(-4*a**2)/(2*exp(-a**2) + exp(-4*a**2))\n",
      "\n",
      "KL expression:  -2.53391278950911*a**2 + log(0.723098356255259*(exp(3*a**2) + 0.5)**1.0)\n",
      "Sympy can only find solutions using complex numbers. So no solution is shown\n",
      "\n",
      "a =  0.577350269189625\n",
      "What value do we get for (p_12 / q_12) and (p_23 / q_23)?\n",
      "p_12 / q_12:  1.00000000000000\n",
      "p_23 / q_23:  0.999999999999997\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "A = 1\n",
    "B = np.sqrt(2)\n",
    "\n",
    "# Distances\n",
    "dp_12 = A\n",
    "dp_13 = A\n",
    "dp_23 = B\n",
    "\n",
    "# Find each of the probabilities p_ij\n",
    "total_sum = np.exp(-dp_12**2) + np.exp(-dp_13**2) + np.exp(-dp_23**2)\n",
    "p_12 = np.exp(-dp_12**2) / total_sum\n",
    "p_13 = np.exp(-dp_13**2) / total_sum\n",
    "p_23 = np.exp(-dp_23**2) / total_sum\n",
    "print(\"p_12: \", p_12)\n",
    "print(\"p_13: \", p_13)\n",
    "print(\"p_23: \", p_23)\n",
    "\n",
    "# Symbolic variables\n",
    "a = Symbol(\"a\", real=True, positive=True)\n",
    "\n",
    "# Distances in the lower dimensional space (1 dimension)\n",
    "dq_12 = a\n",
    "dq_13 = a\n",
    "dq_23 = 2*a\n",
    "\n",
    "# Find each of the probabilities q_ij\n",
    "total_sum = sp.exp(-dq_12**2) + sp.exp(-dq_13**2) + sp.exp(-dq_23**2)\n",
    "q_12 = sp.exp(-dq_12**2) / total_sum\n",
    "q_13 = sp.exp(-dq_13**2) / total_sum\n",
    "q_23 = sp.exp(-dq_23**2) / total_sum\n",
    "print(\"q_12: \", q_12)\n",
    "print(\"q_13: \", q_13)\n",
    "print(\"q_23: \", q_23)\n",
    "print()\n",
    "\n",
    "# Calculate the KL expression\n",
    "KL_pq = ( p_12 * sp.ln(p_12 / q_12) ) + ( p_13 * sp.ln(p_13 / q_13) )+ ( p_23 * sp.ln(p_23 / q_23) )\n",
    "print(\"KL expression: \", KL_pq.simplify())\n",
    "\n",
    "# Solve for a\n",
    "solution = sp.solveset(KL_pq.simplify(), a)\n",
    "print(\"Sympy can only find solutions using complex numbers. So no solution is shown\")\n",
    "print()\n",
    "\n",
    "# How about this trick? \n",
    "# The kullback-leibler divergence is set equal to 0. This happens if each of the \n",
    "# natural logarithms in the KL expression are equal to 0. This happens if the value\n",
    "# inside the logarithm is equal to 1. This happens whenever the numerator and denominator\n",
    "# of the logarithm are equal, so, by setting the numerator and denominator equal to each other,\n",
    "# we can solve for a. The values of (p_12 / q_12) and (p_23 / q_23) are equal to each other,\n",
    "# so if we get the same value for a for the (p_12 / q_12) equation and the (p_23 / q_23) equation,\n",
    "# then we have found the correct value for a.\n",
    "sol_a1 = sp.solve(q_12 - p_12, a)\n",
    "sol_a2 = sp.solve(q_23 - p_23, a)\n",
    "\n",
    "# Are the two solutions equal?\n",
    "if sol_a1 == sol_a2 and len(sol_a1) > 0:\n",
    "    print( \"a = \", sol_a1[0] )\n",
    "    print(\"What value do we get for (p_12 / q_12) and (p_23 / q_23)?\")\n",
    "    print(\"p_12 / q_12: \", p_12 / q_12.subs(a, sol_a1[0]) )\n",
    "    print(\"p_23 / q_23: \", p_23 / q_23.subs(a, sol_a1[0]) )\n",
    "else:\n",
    "    print(\"No solution found. Solution may be empty\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. T-SNE\n",
    "\n",
    "![Alt text](Images/t-sne.PNG)\n",
    "\n",
    "Simplified definition because the real definition includes the variances for each of the dimensions. Here we just assume a unit variance for simplicity sake.\n",
    "\n",
    "#### t-SNE First Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p_12:  0.42231879825151825\n",
      "p_13:  0.42231879825151825\n",
      "p_23:  0.15536240349696354\n",
      "q_12:  (4*a**2 + 1)/(3*(3*a**2 + 1))\n",
      "q_13:  (4*a**2 + 1)/(3*(3*a**2 + 1))\n",
      "q_23:  (a**2 + 1)/(3*(3*a**2 + 1))\n",
      "\n",
      "a1 =  [1.15784634184208]\n",
      "a2 =  [1.15784634184208]\n"
     ]
    }
   ],
   "source": [
    "# Constants\n",
    "A = 1\n",
    "B = np.sqrt(2)\n",
    "\n",
    "# Distances\n",
    "dp_12 = A\n",
    "dp_13 = A\n",
    "dp_23 = B\n",
    "\n",
    "# Find each of the probabilities p_ij\n",
    "total_sum = np.exp(-dp_12**2) + np.exp(-dp_13**2) + np.exp(-dp_23**2)\n",
    "p_12 = np.exp(-dp_12**2) / total_sum\n",
    "p_13 = np.exp(-dp_13**2) / total_sum\n",
    "p_23 = np.exp(-dp_23**2) / total_sum\n",
    "print(\"p_12: \", p_12)\n",
    "print(\"p_13: \", p_13)\n",
    "print(\"p_23: \", p_23)\n",
    "\n",
    "# Symbolic variables\n",
    "a = Symbol(\"a\", real=True, positive=True)\n",
    "\n",
    "# Distances in the lower dimensional space (1 dimension)\n",
    "dq_12 = a\n",
    "dq_13 = a\n",
    "dq_23 = 2*a\n",
    "\n",
    "# Find each of the probabilities q_ij\n",
    "total_sum = (1 / (1 + dq_12**2)) + (1 / (1 + dq_13**2)) + (1 / (1 + dq_23**2))\n",
    "q_12 = (1 / (1 + dq_12**2)) / total_sum\n",
    "q_13 = (1 / (1 + dq_13**2)) / total_sum\n",
    "q_23 = (1 / (1 + dq_23**2)) / total_sum\n",
    "print(\"q_12: \", q_12.simplify())\n",
    "print(\"q_13: \", q_13.simplify())\n",
    "print(\"q_23: \", q_23.simplify())\n",
    "print()\n",
    "\n",
    "# Solve q_12 = p_12 or q_23 = p_23\n",
    "sol_a1 = sp.solve(q_12 - p_12, a)\n",
    "sol_a2 = sp.solve(q_23 - p_23, a)\n",
    "print(\"a1 = \", sol_a1)\n",
    "print(\"a2 = \", sol_a2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis-ayydRhdv-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "71bf549c414c7d2d83e2bde045de550cc634ed7a909cf9e92717235840316976"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
