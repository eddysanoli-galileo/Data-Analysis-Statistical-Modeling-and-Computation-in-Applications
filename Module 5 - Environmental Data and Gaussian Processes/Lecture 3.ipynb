{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3 - Model Selection and Long Range Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sympy as sp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Beyond Linear Dependencies\n",
    "\n",
    "#### Entropy of a Uniform Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy of x:  2.0\n"
     ]
    }
   ],
   "source": [
    "# Probability distribution\n",
    "px = 0.25\n",
    "\n",
    "# Values of x\n",
    "x = [1, 2, 3, 4]\n",
    "\n",
    "# Entropy using the base 2 logarithm (if it uses log2, it is in bits)\n",
    "Hx = 0\n",
    "for i in x:\n",
    "    Hx += px * np.log2(px)\n",
    "Hx = -Hx\n",
    "\n",
    "print('Entropy of x: ', Hx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entropy of a Joint Distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2(X, Y) =  1.5\n"
     ]
    }
   ],
   "source": [
    "# Values of x and y\n",
    "x = [0, 1]\n",
    "y = [0, 1]\n",
    "\n",
    "\n",
    "def P(x, y):\n",
    "    \"\"\"\n",
    "    Joint probability distribution\n",
    "    \"\"\"\n",
    "    if x == 0 and y == 0:\n",
    "        return 0.5\n",
    "    elif x == 0 and y == 1:\n",
    "        return 0\n",
    "    elif x == 1 and y == 0:\n",
    "        return 0.25\n",
    "    elif x == 1 and y == 1:\n",
    "        return 0.25\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Entropy of x and y\n",
    "Hxy = 0\n",
    "for i in x:\n",
    "    for j in y:\n",
    "\n",
    "        # If the log of the probability is minus infinity (log2(0) = -inf), then we assume that\n",
    "        # log2(0) = 0. This is thanks to the fact that lim_{z->0} z*log2(z) = 0\n",
    "        if P(i, j) == 0:\n",
    "            Hxy += 0\n",
    "\n",
    "        # In any other case, compute the joint entropy\n",
    "        else:\n",
    "            Hxy += P(i, j) * np.log2(P(i, j))\n",
    "\n",
    "# Apply the negative sign outside the sum\n",
    "Hxy = -Hxy\n",
    "\n",
    "print('H2(X, Y) = ', Hxy)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conditional Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H2(Y) =  0.8112781244591328\n",
      "H2(X|Y) =  0.6887218755408672\n"
     ]
    }
   ],
   "source": [
    "# Add up the probabilities of y\n",
    "def Py(y):\n",
    "    \"\"\"\n",
    "    Marginal probability distribution of y. We obtain it by summing\n",
    "    up the probabilities of P(x, y) for common values of y\n",
    "    \"\"\"\n",
    "    if y == 0:\n",
    "        return 0.75\n",
    "    elif y == 1:\n",
    "        return 0.25\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Entropy of y\n",
    "Hy = 0\n",
    "for i in y:\n",
    "    Hy += Py(i) * np.log2(Py(i))\n",
    "Hy = -Hy\n",
    "print('H2(Y) = ', Hy)\n",
    "\n",
    "# What is the conditional entropy of X given Y?\n",
    "Hx_given_y = Hxy - Hy\n",
    "print('H2(X|Y) = ', Hx_given_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mutual Information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X =  [1, 2, -1, -2]\n",
      "P(X) =  [0.25, 0.25, 0.25, 0.25]\n",
      "H2(X) =  2.0\n",
      "H2(X, Y) =  2.0\n",
      "H2(Y) =  1.0\n",
      "H2(X|Y) =  1.0\n",
      "I(X;Y) =  1.0\n"
     ]
    }
   ],
   "source": [
    "# Possible values for Z and Y\n",
    "z = [1, -1]\n",
    "y = [1, 2]\n",
    "\n",
    "# Probability distribution of Z and Y\n",
    "Pz = 0.5\n",
    "Py = 0.5\n",
    "\n",
    "# X = Y * Z. We compute all possible values of X\n",
    "x = []\n",
    "for i in z:\n",
    "    for j in y:\n",
    "        x.append(i * j)\n",
    "print('X = ', x)\n",
    "\n",
    "# Probability distribution of X\n",
    "Px = [Pz * Py] * 4\n",
    "print('P(X) = ', Px)\n",
    "\n",
    "# Entropy of X\n",
    "Hx = 0\n",
    "for i in Px:\n",
    "    Hx += i * np.log2(i)\n",
    "Hx = -Hx\n",
    "print('H2(X) = ', Hx)\n",
    "\n",
    "\n",
    "def Pxy(x, y):\n",
    "    \"\"\"\n",
    "    Joint probability distribution of X and Y\n",
    "\n",
    "    -----------------------------------\n",
    "    | Y/X  | -2   | -1   |  1   |  2   |\n",
    "    |------|------|------|------|------|\n",
    "    |  1   |  0   | 1/4  | 1/4  | 0    |\n",
    "    |------|------|------|------|------|\n",
    "    |  2   |  1/4 | 0    | 0    | 1/4  |\n",
    "    ------------------------------------\n",
    "    \"\"\"\n",
    "    if x == -2 and y == 2:\n",
    "        return 0.25\n",
    "    elif x == -1 and y == 1:\n",
    "        return 0.25\n",
    "    elif x == 1 and y == 1:\n",
    "        return 0.25\n",
    "    elif x == 2 and y == 2:\n",
    "        return 0.25\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "\n",
    "# Joint entropy of X and Y\n",
    "Hxy = 0\n",
    "for i in x:\n",
    "    for j in y:\n",
    "        if Pxy(i, j) == 0:\n",
    "            Hxy += 0\n",
    "        else:\n",
    "            Hxy += Pxy(i, j) * np.log2(Pxy(i, j))\n",
    "Hxy = -Hxy\n",
    "print('H2(X, Y) = ', Hxy)\n",
    "\n",
    "# Entropy of Y\n",
    "Hy = 0\n",
    "for i in y:\n",
    "    Hy += Py * np.log2(Py)\n",
    "Hy = -Hy\n",
    "print('H2(Y) = ', Hy)\n",
    "\n",
    "# Conditional entropy of X given Y\n",
    "Hx_given_y = Hxy - Hy\n",
    "print('H2(X|Y) = ', Hx_given_y)\n",
    "\n",
    "# Mutual information\n",
    "Ixy = Hx - Hx_given_y\n",
    "print('I(X;Y) = ', Ixy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_analysis-ayydRhdv-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
